#
# Copyright (c) 2021 The banded_matrices Contributors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
import numpy as np
import pytest
import tensorflow as tf

from banded_matrices.banded import (
    cholesky_band,
    product_band_band,
    square_band,
    symmetrise_band,
    transpose_band,
)
from tests.utils.banded_matrices_utils import (
    constant_op,
    generate_banded_tensor,
    to_dense_tensor,
)


@pytest.mark.parametrize(
    "shape_with_bands_left, shape_with_bands_right",
    [
        # Each tuple has:
        # (dimensions to broadcast,  .... , lower_band, upper_band, square matrix dimension)
        # Stacked banded matrices on the left:
        ((5, 2, 3, 20), (2, 3, 20)),  # One level of stacking
        ((2, 2, 2, 3, 20), (2, 1, 20)),  # Two levels of stacking
        ((1, 1, 1, 2, 3, 20), (1, 2, 20)),  # Three levels of stacking
        # Stacked banded matrices on the right:
        ((2, 3, 20), (5, 2, 3, 20)),  # One level of stacking
        ((2, 3, 20), (2, 1, 2, 3, 20)),  # Two levels of stacking
        # Both left and right stack the same number of matrices to multiply pairwise:
        ((5, 2, 3, 20), (5, 4, 0, 20)),  # One level of stacking
        ((2, 2, 2, 3, 20), (2, 2, 4, 0, 20)),  # Two levels of stacking
        ((2, 1, 2, 2, 3, 20), (2, 1, 2, 4, 0, 20)),  # Three levels of stacking
        # Both left and right stack with different but broadcastable levels:
        ((5, 2, 3, 20), (1, 4, 0, 20)),  # One level of stacking
        ((5, 2, 3, 20), (4, 0, 20)),  # One level of stacking, implicit dimension trails
        ((2, 2, 2, 3, 20), (2, 1, 4, 0, 20)),  # Two levels of stacking
        ((4, 1, 2, 3, 20), (2, 1, 3, 4, 0, 20)),  # Two/Three levels of stacking
    ],
)
def test_banded_product_broadcast(shape_with_bands_left, shape_with_bands_right):
    """
    Tests that banded product broadcasts like numpy product.
    """
    with tf.Graph().as_default():

        l1, u1, n = shape_with_bands_left[-3:]
        l2, u2, check_n = shape_with_bands_right[-3:]
        assert check_n == n

        banded1 = generate_banded_tensor(shape_with_bands_left)
        banded2 = generate_banded_tensor(shape_with_bands_right)

        dense1 = to_dense_tensor(banded1, l1, u1)
        dense2 = to_dense_tensor(banded2, l2, u2)

        dense_product = dense1 @ dense2
        assert dense_product.shape[-2:] == (n, n)

        cst_op1 = constant_op(banded1)
        cst_op2 = constant_op(banded2)

        product = product_band_band(
            cst_op1,
            cst_op2,
            left_lower_bandwidth=l1,
            left_upper_bandwidth=u1,
            right_lower_bandwidth=l2,
            right_upper_bandwidth=u2,
        )

        assert product.shape[-2:] == (l1 + l2 + 1 + u1 + u2, n)

        with tf.compat.v1.Session() as session:
            computed = session.run(product)
            computed = to_dense_tensor(computed, l1 + l2, u1 + u2)
            np.testing.assert_allclose(computed, dense_product)


@pytest.mark.parametrize(
    "op",
    [
        transpose_band,
        square_band,
        # We could test `unpack_banded_matrix_to_dense` as well here but it would need to generate
        # grad_ys that carefully have 0s out of the band - the operator correctly, but annoyingly,
        # refuses to work with the gradient generated by default in tests that has 1s everywhere.
    ],
)
def test_unary_operators_broadcast(op):
    """
    Checks that a broadcasting operator called on stacked banded matrices works and that its
    result works consistently with the non-broadcasting version (checked on a single stacked
    band).

    Here we consider unary operators that take a banded matrix and are called similarly.
    """
    with tf.compat.v1.Session(graph=tf.Graph()) as session:
        depth, l, u, n = 2, 3, 4, 15

        band_source = generate_banded_tensor((depth, l, u, n))
        input_stacked = tf.constant(band_source)
        input_first = tf.constant(band_source[0])

        op_stacked = op(input_stacked, l, u)
        op_single = op(input_first, l, u)

        stacked_grad = tf.gradients(ys=op_stacked, xs=input_stacked)
        single_grad = tf.gradients(ys=op_single, xs=input_first)

        np.testing.assert_allclose(session.run(op_stacked)[0], session.run(op_single))
        np.testing.assert_allclose(
            session.run(stacked_grad)[0][0], session.run(single_grad)[0]
        )


@pytest.mark.parametrize("op", [symmetrise_band])
def test_unary_operators_broadcast_lower(op):
    """
    Checks that a broadcasting operator called on stacked banded matrices works and that its
    result works consistently with the non-broadcasting version (checked on a single stacked
    band).

    Here we consider unary operators that take a lower-triangular banded matrix and are called
    similarly.
    """
    with tf.compat.v1.Session(graph=tf.Graph()) as session:
        depth, l, u, n = 2, 3, 0, 15

        band_source = generate_banded_tensor((depth, l, u, n))

        input_stacked = tf.constant(band_source)
        input_first = tf.constant(band_source[0])

        op_stacked = op(input_stacked, l)
        op_single = op(input_first, l)
        np.testing.assert_allclose(session.run(op_stacked)[0], session.run(op_single))

        # TODO(lucas) restore the following tests when there is a gradient for these ops:
        # stacked_grad = tf.gradients(op_stacked, input_stacked)
        # single_grad = tf.gradients(op_single, input_first)
        # np.testing.assert_allclose(session.run(stacked_grad)[0][0], session.run(single_grad)[0])


def test_cholesky_broadcast():
    """
    Test specifically for Cholesky, here we need to make sure the input is positive definite.
    Here we compare the results against a dense Cholesky, for a single one of the stacked bands.
    """
    with tf.Graph().as_default():
        depth, l, u, n = 2, 3, 0, 15

        banded = generate_banded_tensor((depth, l, u, n), ensure_positive_definite=True)
        dense = to_dense_tensor(banded, l, u)

        cst_op_banded = tf.constant(banded)
        # TF Cholesky does not broadcast so we just check the first stacked vector
        cst_op_dense = tf.constant(dense[0])

        cholesky_banded = cholesky_band(cst_op_banded)
        cholesky_dense = tf.linalg.cholesky(cst_op_dense)

        with tf.compat.v1.Session() as session:
            computed = session.run(cholesky_banded)[0]
            dense_reference = session.run(cholesky_dense)
            computed = to_dense_tensor(computed, l, u)
            np.testing.assert_allclose(computed, dense_reference)


def test_cholesky_broadcast_deep():
    """
    Test for Cholesky broadcasting. Here we test more than 1 (i.e. 2) levels of stacking.
    """
    with tf.Graph().as_default():
        first_stacking, second_stacking, l, u, n = 2, 1, 4, 0, 16

        banded = generate_banded_tensor(
            (first_stacking, second_stacking, l, u, n), ensure_positive_definite=True
        )
        dense = to_dense_tensor(banded, l, u)

        cst_op_banded = tf.constant(banded)
        cst_op_dense = tf.constant(dense[0][0])

        cholesky_banded = cholesky_band(cst_op_banded)
        cholesky_dense = tf.linalg.cholesky(cst_op_dense)

        banded_grad = tf.gradients(ys=cholesky_banded, xs=cst_op_banded)
        dense_grad = tf.gradients(ys=cholesky_dense, xs=cst_op_dense)

        with tf.compat.v1.Session() as session:

            computed = session.run(cholesky_banded)[0][0]
            dense_reference = session.run(cholesky_dense)
            computed = to_dense_tensor(computed, l, u)

            computed_grad = session.run(banded_grad)[0][0]
            dense_grad = session.run(dense_grad)
            computed_grad = to_dense_tensor(computed_grad, l, u)

            np.testing.assert_allclose(computed, dense_reference)
            # np.testing.assert_allclose(np.diag(computed_grad[0]), np.diag(dense_grad[0]))


def test_banded_product_broadcast_gradient():
    """
    Tests that gradient can be called and does a broadcast version of the basic 2D operator.
    """
    shape_with_bands_left, shape_with_bands_right = ((5, 2, 3, 20), (2, 3, 20))

    with tf.Graph().as_default():
        l1, u1, n = shape_with_bands_left[-3:]
        l2, u2, check_n = shape_with_bands_right[-3:]
        assert check_n == n

        banded1 = generate_banded_tensor(shape_with_bands_left)
        banded2 = generate_banded_tensor(shape_with_bands_right)

        cst_op1 = constant_op(banded1)
        cst_op2 = constant_op(banded2)

        cst_op1_at0 = cst_op1[0]

        # We mix call by position and by name on the first arguments to exercise the decorator:
        product = product_band_band(
            left=cst_op1,
            right=cst_op2,
            left_lower_bandwidth=l1,
            left_upper_bandwidth=u1,
            right_lower_bandwidth=l2,
            right_upper_bandwidth=u2,
        )

        product_0 = product_band_band(
            cst_op1_at0,
            right=cst_op2,
            left_lower_bandwidth=l1,
            left_upper_bandwidth=u1,
            right_lower_bandwidth=l2,
            right_upper_bandwidth=u2,
        )

        full_grad = tf.gradients(ys=product, xs=cst_op1)
        grad_0 = tf.gradients(ys=product_0, xs=cst_op1_at0)

        assert product.shape[-2:] == (l1 + l2 + 1 + u1 + u2, n)

        with tf.compat.v1.Session() as session:
            computed = session.run(product)
            computed_0 = session.run(product_0)
            np.testing.assert_allclose(computed[0], computed_0)

            g = session.run(full_grad)[0]
            g0 = session.run(grad_0)[0]
            np.testing.assert_allclose(g[0], g0)
